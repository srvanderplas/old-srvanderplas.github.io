%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author: Susan Vanderplas
%
% This is an example of a complete CV using the 'moderncv' package
% and the 'timeline' package. For more information on those, please
% access:
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderntimeline
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderncv
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PassOptionsToPackage{pdfpagelabels=false}{hyperref}
\documentclass[12pt, letterpaper, sans]{moderncv}
\moderncvstyle{classic}
\moderncvcolor{blue}
\usepackage[utf8]{inputenc}
\usepackage[scale=0.85]{geometry}    % Width of the entire CV
\setlength{\hintscolumnwidth}{1.5in} % Width of the timeline on your left
\usepackage{pdfpages}
\usepackage{moderntimeline}
\usepackage{xpatch}
\usepackage{color, graphicx}
\usepackage[unicode,]{hyperref}
\usepackage{xcolor}
\usepackage{varwidth}
\definecolor{link}{HTML}{3873B3}
\hypersetup{colorlinks, breaklinks,
            linkcolor=link,
            urlcolor=link,
            citecolor=link}

\makeatletter
\newcommand{\makesimpletitle}{%
 % recompute lengths (in case we are switching from letter to resume, or vice versa)
  \recomputeletterlengths%

  \begin{varwidth}[c]{.75\textwidth}
  \if@left\raggedright\fi%
      \if@right\raggedleft\fi%
      % \namestyle{\@firstname\ \@lastname}%
      \ifthenelse{\equal{\@title}{}}{}{\titlestyle{\@title}}%
  \end{varwidth}\hfill
  \begin{varwidth}[c]{.25\textwidth}%
    % optional detailed information
      \raggedleft%
      \addressfont\textcolor{color2}{%
        {\bfseries\upshape\@firstname~\@lastname}\\
        % optional detailed information
        \ifthenelse{\isundefined{\@addressstreet}}{}{\makenewline\addresssymbol\@addressstreet%
          \ifthenelse{\equal{\@addresscity}{}}{}{\makenewline\@addresscity}% if \addresstreet is defined, \addresscity and addresscountry will always be defined but could be empty
          \ifthenelse{\equal{\@addresscountry}{}}{}{\makenewline\@addresscountry}}%
        \collectionloop{phones}{% the key holds the phone type (=symbol command prefix), the item holds the number
          \makenewline\csname\collectionloopkey phonesymbol\endcsname\collectionloopitem}%
        \ifthenelse{\isundefined{\@email}}{}{\makenewline\emailsymbol\emaillink{\@email}}%
        \ifthenelse{\isundefined{\@homepage}}{}{\makenewline\homepagesymbol\httplink{\@homepage}}%
        \ifthenelse{\isundefined{\@extrainfo}}{}{\makenewline\@extrainfo}}
    \end{varwidth}
}
\makeatother



% Personal Information
\name{Susan}{Vanderplas}
\title{\emph{Research and Teaching Statement}}
\address{801 Onyx Cir}{Ames, IA 50010}{}
\phone[mobile]{515-509-6613}
%\phone[fixed]{+55~(11)~3091~2722}
% \email{srvanderplas@gmail.com}                % optional, remove / comment the line if not wanted
\email{srvander@iastate.edu}                % optional, remove / comment the line if not wanted
% \homepage{srvanderplas.com}                   % optional, remove / comment the line if not wanted
%\social[linkedin]{}                          % optional, remove / comment the line if not wanted
%\social[twitter]{srvanderplas}               % optional, remove / comment the line if not wanted
\social[github]{srvanderplas}                 % optional, remove / comment the line if not wanted
%\extrainfo{\emailsymbol \emaillink{}}


\begin{document}
\makesimpletitle

\section{Research}

My primary research interests fall into a ``grey area" between statistics, machine learning, and cognitive science. Specifically, I explore ways to increase our ability to understand data, using data visualization and visual statistics; at the same time, I design and use algorithms intended to mimic areas where humans perform better than computers, such as object recognition. This research is inherently interdisciplinary - in order to design algorithms that mimic the human visual system, it is necessary to understand the architecture human visual system in addition to parameter estimation techniques and ways to transform data into relevant feature sets. This area of research has broad applications: statistics and data visualization, but also domain-specific graphics in bioinformatics and engineering, as well as algorithm development and human factors in forensics. My current research in forensics is focused on two areas related to human-statistics interaction, with additional projects in areas related to simulation and reproducible research. 

% Human-like algorithms, assessment of man vs. machine performance
The first human-statistics interaction area is in the development and use of human-like machine learning algorithms for forensic pattern evidence. Pattern matching is one area where humans excel and computers are still catching up; however, computer algorithms have advantages in forensics: they are repeatable, auditable, and their failure rates can be determined with relative ease. In addition, algorithms which are perceived to mimic examiners can be explained by analogy to juries and to practitioners; they may allow for an understanding of the process while requiring relatively little statistical theory or notation. I am currently using computer-vision techniques to create feature sets for images of shoes; these feature sets can then be used for modeling, to speed up database searches, or to assess the frequency of certain class characteristics in a local population. In the next 6 months, I plan to use a similar approach to to identify regions of bullets suitable for striae matching, which will increase the effectiveness of the matching algorithm by excluding degraded areas from match scores. I also am planning to compare the firearms examiner's process for bullet matching to the automated matching algorithm to assess the matching algorithm's convergent validity; this may make examiners more comfortable with the automated process and provide additional justification for its use in legal proceedings. 

% Human factors, truthiness, presentation of 
The second human-statistics interaction area I am interested in is what is termed `human factors' in forensics. Specifically, there is concern over the ability of juries and legal professionals to understand and appropriately utilize statistical concepts such as likelihood ratios, Bayes Factors, and error rates to evaluate the weight of forensic evidence. I am laying groundwork to assess the effect of charts in legal proceedings with an experiment evaluating how individuals use pictures, charts, and maps to evaluate factual claims. Once we understand how visual aids affect evaluation of generic factual claims, future experiments can add more legal context and examine jurors specifically. I am also collaborating with Danica Ommen on a grant exploring the relationship between ROC curves, likelihood ratios, and Bayes Factors. I am responsible for developing appropriate and effective graphics for use by statisticians, examiners, and potentially in legal proceedings. Forensic science needs statistics, but we also need to ensure that the statistical information is communicated effectively, so that it can be fairly evaluated and used to improve the justice system instead of introducing a new source of bias. 

% Simulation and data processing
Forensic data, at least as collected at CSAFE, tends to be large, both in size and complexity. Often, this data takes the form of two or three dimensional images, which must then be broken down into meaningful features before statistical analysis can begin. In many situations, it is more economical to generate simulated data rather than collecting it. For instance, to characterize the random match probability of a type of pattern evidence, it may be easier to generate a million patterns than to collect the same amount of data experimentally. While it may be infeasible to generate simulated data at the level it is collected, it is in many cases possible to generate simulated data after some or all of the initial processing is completed. For bullets, for example, it is impractical to generate multiple lands worth of 3D scan data that adequately mimics characteristics found in actual scanned data, but it is possible to generate land signatures (cross-sections with the curve removed) that mimic signatures found in collected data. I am currently working on a bootstrap technique for simulating known match and known non-match signatures; I should be able to use that to determine the probability of a random match as well as the distribution of features and match scores under known match and known non-match distributions. More generally, I am broadly interested in efficiently generating synthetic data for estimating score distributions. 

% Data science and software
Software used in forensic examination should meet certain criteria - it should be auditable (source code available), it should be tested (unit test coverage above 95\%, ideally), any changes should be logged (version control), and any deviations from behavior of prior versions should be explicitly noted and justified. Unfortunately, a large percentage of software products currently in use in forensics are closed-source, and versioning information, testing information, and changes in software behavior are not available to experts or to the general public. There's an education and outreach component to this topic, but there is also a research component: while the software versioning is relatively well handled, versioning statistical models requires handling both data and code. Some work on this has been done in the reproducible research push within data science, but few solutions handle initial and intermediate data products along with code versioning. I have done some work on dynamic updating of model results when input data or software changes, and plan to expand this into a more general framework for tracking the provenance of statistical models and results, producing a `chain of evidence' like record for modeling. I am currently working on a paper describing best practices for forensics software, and expect to expand this to include new strategies for handling data along with software. 




% 
% The phrase ``A picture is worth a thousand words" is commonly accepted, in part because the human brain has been optimized over millenia to process visual stimuli quickly and efficiently. We can communicate information much more quickly and effectively using images, which provides a substantial advantage to graphical communication relative to verbal communication methods. This advantage comes with certain drawbacks: statisticians must consider the visual system in order to communicate effectively with charts and graphs. John Tukey and Edward Tufte both described guidelines for data visualization, but there is fairly sparse literature examining the perception of statistical graphics in an experimental setting. My doctoral research considered different situations in which features of a chart or graph might change the conclusion drawn from that chart, even when the underlying data remain the same.
% 
% As statistics becomes more important to the world of forensic science, we have to be able to communicate the results of statistical evaluation of forensic evidence to experts in other fields (lawyers, examiners, judges) as well as to laypeople (juries). If the experts do not understand the statistics, they won't be adopted; if juries cannot understand the statistics, there is little point to using them. Graphics become a valuable tool because they can demonstrate information spatially without requiring the overhead of a complete theoretical understanding of likelihood ratios, hypothesis testing, or evaluation of machine learning algorithms. I am currently working to lay groundwork for studies of how juries evaluate graphs and charts by first understanding how laypeople evaluate factual claims in the presence of pictures, charts, and maps which are either related or unrelated to the factual claim. I am also collaborating on a grant exploring the connection between likelihood ratios, bayes factors, and ROC curves; my responsibility is to create and assess graphics which can be used in legal proceedings to effectively communicate the strength of the evidence to juries. In the future, I hope to collaborate with researchers in the psychology department to assess graphics presented within the context of legal proceedings, with the goal of developing guidelines for both the types of graphics and the languaged used to present them.
% 
% There are certain tasks which humans can still do more effectively than algorithms; for example, object identification, pattern recognition, anomaly detection. In recent years, algorithms have been developed to mimic the architecture of parts of the brain, combining human strengths with computational reproducibility. Convolutional neural networks (CNNs) have become the de facto standard for object recognition in machine learning because they mimic the visual cortex. In forensics, there are certain tasks which are still difficult to automate: identification of randomly acquired marks on shoes, assessment of whether a bullet is suitable for comparison, and labeling of fingerprint minutiae, to name a few. I am working to train a CNN to recognize class characteristics in shoe treads - the resulting identified characteristics can then be used to index shoe models in a database of exemplars; they can also be used to characterize the frequency of certain shoe design elements within a local population. In the coming months, I intend to apply a similar technique to identify areas on bullet scans which are degraded due to pitting, tank rash, or other phenomena. Output from this algorithm can then be used as input to the algorithm used to identify a stable cross section for identification of striae. 
% 
% Even though neural networks are `black box' models, and make statisticians uncomfortable due to their opacity, they can be explained to laypeople by analogy, avoiding the bulk of technical jargon. We need to be able to explain our other algorithms in a similar way, so that practitioners, lawyers, and judges can trust that the statistical algorithms are using data in a way that is similar to the status quo, with additional advantages of being repeatable, auditable, and quantitative. To facilitate this, we need to gather data on what features the examiners use and their importance, and compare those features to the variables used in machine learning models. Predictions from machine learning models can be examined using LIME (local interpretable model-agnostic explanations); on the examiner side, we can use eye-tracking and other tools to gain insight into the features of the evidence used to make a determination. In addition to providing information on the correspondence between the algorithm and the examiner, this investigation may also provide additional features for the next generation of statistical models. Initially, I plan to compare predictions made by the bullet matching algorithm developed at CSAFE with the process used by firearms examiners. I will collect information about the features examiners use to make a determination using a combination of eye tracking, think-aloud protocols, and a virtual comparison microscope; this data will be compared to local features in order to assess the procedural similarity between human and algorithmic approaches. 
% 
% Finally, in any of the pattern evidence areas of forensics, there is a need to obtain sufficient data to characterize the distribution of match scores under known match and known non-match conditions. Typically, this data is difficult, expensive, or time consuming to obtain, so limited sets of data are used to explore the match and non-match distributions. It may not be possible to simulate the input data fully (e.g. generate a 3D simulated bullet scan or shoe print with random characteristics), but for certain stages of any matching algorithm, simulated data may provide additional information about the score distributions. I am currently working on a method to simulate matching and non-matching bullet land signatures using a common examiner test dataset, but I would like to expand that method so that it can be generalized to multiple ammunition and firearm types. To expand the threshold bootstrap method to allow generation of signatures from different ammunition types, it will be necessary to assemble a database of firearm and ammunition types that can be used to characterize the striae features of each combination. From this database, it should then be possible to develop a method by which certain types of striae can be sampled, scaled, and modified, then reassembled into either a set of matching signatures or a set of non-matching signatures. 





\section{Teaching}

\section{Diversity and Inclusion}


\end{document}