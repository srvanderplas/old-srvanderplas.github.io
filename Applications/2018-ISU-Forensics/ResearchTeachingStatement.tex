%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author: Susan Vanderplas
%
% This is an example of a complete CV using the 'moderncv' package
% and the 'timeline' package. For more information on those, please
% access:
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderntimeline
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderncv
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PassOptionsToPackage{pdfpagelabels=false}{hyperref}
\documentclass[12pt, letterpaper, sans]{moderncv}
\moderncvstyle{classic}
\moderncvcolor{blue}
\usepackage[utf8]{inputenc}
\usepackage[scale=0.85]{geometry}    % Width of the entire CV
\setlength{\hintscolumnwidth}{1.5in} % Width of the timeline on your left
\usepackage{pdfpages}
\usepackage{moderntimeline}
\usepackage{xpatch}
\usepackage{color, graphicx}
\usepackage[unicode,]{hyperref}
\usepackage{xcolor}
\usepackage{varwidth}
\definecolor{link}{HTML}{3873B3}
\hypersetup{colorlinks, breaklinks,
            linkcolor=link,
            urlcolor=link,
            citecolor=link}

\makeatletter
\newcommand{\makesimpletitle}{%
 % recompute lengths (in case we are switching from letter to resume, or vice versa)
  \recomputeletterlengths%

  \begin{varwidth}[c]{.75\textwidth}
  \if@left\raggedright\fi%
      \if@right\raggedleft\fi%
      % \namestyle{\@firstname\ \@lastname}%
      \ifthenelse{\equal{\@title}{}}{}{\titlestyle{\@title}}%
  \end{varwidth}\hfill
  \begin{varwidth}[c]{.25\textwidth}%
    % optional detailed information
      \raggedleft%
      \addressfont\textcolor{color2}{%
        {\bfseries\upshape\@firstname~\@lastname}\\
        % optional detailed information
        \ifthenelse{\isundefined{\@addressstreet}}{}{\makenewline\addresssymbol\@addressstreet%
          \ifthenelse{\equal{\@addresscity}{}}{}{\makenewline\@addresscity}% if \addresstreet is defined, \addresscity and addresscountry will always be defined but could be empty
          \ifthenelse{\equal{\@addresscountry}{}}{}{\makenewline\@addresscountry}}%
        \collectionloop{phones}{% the key holds the phone type (=symbol command prefix), the item holds the number
          \makenewline\csname\collectionloopkey phonesymbol\endcsname\collectionloopitem}%
        \ifthenelse{\isundefined{\@email}}{}{\makenewline\emailsymbol\emaillink{\@email}}%
        \ifthenelse{\isundefined{\@homepage}}{}{\makenewline\homepagesymbol\httplink{\@homepage}}%
        \ifthenelse{\isundefined{\@extrainfo}}{}{\makenewline\@extrainfo}}
    \end{varwidth}
}
\makeatother



% Personal Information
\name{Susan}{Vanderplas}
\title{\emph{Research and Teaching Statement}}
\address{801 Onyx Cir}{Ames, IA 50010}{}
\phone[mobile]{515-509-6613}
%\phone[fixed]{+55~(11)~3091~2722}
% \email{srvanderplas@gmail.com}                % optional, remove / comment the line if not wanted
\email{srvander@iastate.edu}                % optional, remove / comment the line if not wanted
% \homepage{srvanderplas.com}                   % optional, remove / comment the line if not wanted
%\social[linkedin]{}                          % optional, remove / comment the line if not wanted
%\social[twitter]{srvanderplas}               % optional, remove / comment the line if not wanted
\social[github]{srvanderplas}                 % optional, remove / comment the line if not wanted
%\extrainfo{\emailsymbol \emaillink{}}


\begin{document}
\makesimpletitle

\section{Research}

My primary research interests fall into a ``grey area" between statistics, machine learning, and cognitive science. Specifically, I explore ways to increase our ability to understand data, using data visualization and visual statistics; at the same time, I design and use algorithms intended to mimic areas where humans perform better than computers, such as object recognition. This research is inherently interdisciplinary - in order to design algorithms that mimic the human visual system, it is necessary to understand the architecture human visual system in addition to parameter estimation techniques and ways to transform data into relevant feature sets. This area of research has broad applications: statistics and data visualization, but also domain-specific graphics in bioinformatics and engineering, as well as algorithm development and human factors in forensics. My current research is forensics focused, but much of it has broader applications in data science. I am currently working in two areas related to human-statistics interaction, with additional projects in areas related to simulation and reproducible research. 

% Human-like algorithms, assessment of man vs. machine performance
The first human-statistics interaction area is in the development and use of human-like machine learning algorithms for forensic pattern evidence and other image data. Pattern matching is one area where humans excel and computers are still catching up, but computer algorithms have important advantages over humans. Computational methods produce results that are repeatable, auditable, and whose failure rates are generally easy to determine. In addition, algorithms which are perceived to mimic human judgment can be explained by analogy to laypeople and practitioners; they may allow for an understanding of the process while requiring relatively little statistical theory or notation. I am currently using computer-vision techniques to create feature sets for images of shoes; these feature sets can then be used for modeling, to speed up database searches, or to assess the frequency of certain class characteristics in a local population. In the near future, I plan to use a similar approach to to identify regions of bullets suitable for striae matching, which will increase the effectiveness of the matching algorithm by excluding degraded areas from match scores. Additionally, I am planning a related study to compare the firearms examiner's process for bullet matching to the automated matching algorithm, assessing the matching algorithm's convergent validity; this may make examiners more comfortable with the automated process and provide additional justification for its use in legal proceedings. 

% Human factors, truthiness, presentation of 
The second human-statistics interaction area I am interested in is what is termed `human factors' in forensics. Specifically, there is concern over the ability of juries and legal professionals to understand and appropriately utilize statistical concepts such as p-values, likelihood ratios, and error rates to evaluate the weight of forensic evidence. Similar issues have surfaced in medical fields, where graphics showing many stick figures are used to provide easier explanations of test error rates. I am laying groundwork to assess the effect of charts in legal proceedings with an experiment evaluating how individuals use pictures, charts, and maps to evaluate factual claims. Once we understand how visual aids affect evaluation of generic factual claims, future experiments can add more legal context and examine jurors specifically. I am also collaborating with Danica Ommen on a grant exploring the relationship between ROC curves, likelihood ratios, and Bayes Factors. I am responsible for developing appropriate and effective graphics for use by statisticians, examiners, and laypeople. Forensic science needs statistics, but statisticians need to ensure that the statistical information is communicated effectively, so that it can be fairly evaluated. 

% Simulation and data processing
Data used in machine learning tends to be large in size and may have complicated structure. In forensics, medicine, and astrophysics, this data often takes the form of two or three dimensional images, which must then be broken down into meaningful features before statistical analysis can begin. In many situations, it is more economical to generate simulated data rather than collecting it, and while it may be infeasible to generate simulated data at the level it is collected, it may be possible to generate simulated data after some or all of the initial processing is completed. For bullets, for example, it is impractical to generate multiple lands worth of 3D scan data that adequately mimics characteristics found in actual scanned data, but it is possible to generate land signatures (cross-sections with the curve removed) that mimic signatures found after several data processing and cleaning steps. I am currently working on a bootstrap technique for simulating known match and known non-match signatures; I should be able to use that to determine the probability of a random match as well as the distribution of features and match scores under known match and known non-match distributions. More generally, I am broadly interested in efficiently generating synthetic data for estimating score distributions or assessing algorithm performance. 

% Data science and software
Software used in consequential fields (forensics, medicine, engineering) should meet certain criteria - it should be auditable (source code available), it should be modular (with each module tested separately as well as overall testing), any changes should be logged (version control), and any deviations from behavior of prior versions should be explicitly noted and justified. Unfortunately, a large percentage of software products currently used in these consequential fields are closed-source, and versioning information, testing information, and changes in software behavior are not available to experts or to the general public. Even open-source software is often not tested thoroughly or continuously with each small change. This is an important issue for data science: the tools we create must be reliable, well documented, and well tested. As probability theory is essential to the validity of applied statistics, it is also important that data scientists and statisticians develop a good foundation for the tools and pipelines we build. In particular, it is important that statisticians improve upon the version control philosophy, incorporating not just changes to code, but also changes to data (initial data and downstream intermediate results). I have done some work to build a system for local tracking and conditional updating of intermediate results, and I am working to expand this infrastructure to integrate tracking of code, data, and intermediate results, but I would like to make this framework more general and build tools to make it easier for other researchers to implement similar infrastructure. This may require collaboration with individuals in computer science or software engineering, but should add to the available framework for reproducible research and increase confidence in the results of statistical algorithms in consequential areas where data provenance is critical. 

\section{Teaching}
% Importance of integrating communication, practical programming, and statistical theory into the teaching of data science
% Students need to not only be statistically knowledgable, they also need practical skills in a range of things from server administration to containerization to software engineering practices
% Data scientists that have only worked with clean data are effectively useless in industry. You have to be able to get your own data if it exists, design the experiment to collect it if it doesn't, and then clean it, analyze it, report on it, and present the results, and you have to be able to do all of that at the speed of other business decisions. In news, that's < 24h, in industrial situations, it's on the order of weeks, and in medicine and healthcare it may be months or years because of the overhead involved in clinical trials and data collection.



\section{Diversity and Inclusion}
% Discuss the importance of including ethics in data science courses - algorithms may unintentionally reinforce or perpetuate stereotypical ideas or behavior - policing



\end{document}