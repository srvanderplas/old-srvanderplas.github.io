%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author: Susan Vanderplas
%
% This is an example of a complete CV using the 'moderncv' package
% and the 'timeline' package. For more information on those, please
% access:
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderntimeline
% https://www.ctan.org/tex-archive/macros/latex/contrib/moderncv
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PassOptionsToPackage{pdfpagelabels=false}{hyperref}
\documentclass[12pt, letterpaper, sans]{moderncv}
\moderncvstyle{classic}
\moderncvcolor{blue}
\usepackage[utf8]{inputenc}
\usepackage[scale=0.85]{geometry}    % Width of the entire CV
\setlength{\hintscolumnwidth}{1.5in} % Width of the timeline on your left
\usepackage{pdfpages}
\usepackage{moderntimeline}
\usepackage{xpatch}
\usepackage{color, graphicx}
\usepackage[unicode,]{hyperref}
\usepackage{xcolor}
\usepackage{varwidth}
\definecolor{link}{HTML}{3873B3}
\hypersetup{colorlinks, breaklinks,
            linkcolor=link,
            urlcolor=link,
            citecolor=link}

\makeatletter
\newcommand{\makesimpletitle}{%
 % recompute lengths (in case we are switching from letter to resume, or vice versa)
  \recomputeletterlengths%

  \begin{varwidth}[c]{.75\textwidth}
  \if@left\raggedright\fi%
      \if@right\raggedleft\fi%
      % \namestyle{\@firstname\ \@lastname}%
      \ifthenelse{\equal{\@title}{}}{}{\titlestyle{\@title}}%
  \end{varwidth}\hfill
  \begin{varwidth}[c]{.25\textwidth}%
    % optional detailed information
      \raggedleft%
      \addressfont\textcolor{color2}{%
        {\bfseries\upshape\@firstname~\@lastname}\\
        % optional detailed information
        \ifthenelse{\isundefined{\@addressstreet}}{}{\makenewline\addresssymbol\@addressstreet%
          \ifthenelse{\equal{\@addresscity}{}}{}{\makenewline\@addresscity}% if \addresstreet is defined, \addresscity and addresscountry will always be defined but could be empty
          \ifthenelse{\equal{\@addresscountry}{}}{}{\makenewline\@addresscountry}}%
        \collectionloop{phones}{% the key holds the phone type (=symbol command prefix), the item holds the number
          \makenewline\csname\collectionloopkey phonesymbol\endcsname\collectionloopitem}%
        \ifthenelse{\isundefined{\@email}}{}{\makenewline\emailsymbol\emaillink{\@email}}%
        \ifthenelse{\isundefined{\@homepage}}{}{\makenewline\homepagesymbol\httplink{\@homepage}}%
        \ifthenelse{\isundefined{\@extrainfo}}{}{\makenewline\@extrainfo}}
    \end{varwidth}
}
\makeatother



% Personal Information
\name{Susan}{Vanderplas}
\title{\emph{Research and Teaching Statement}}
\address{801 Onyx Cir}{Ames, IA 50010}{}
\phone[mobile]{515-509-6613}
%\phone[fixed]{+55~(11)~3091~2722}
% \email{srvanderplas@gmail.com}                % optional, remove / comment the line if not wanted
\email{srvander@iastate.edu}                % optional, remove / comment the line if not wanted
% \homepage{srvanderplas.com}                   % optional, remove / comment the line if not wanted
%\social[linkedin]{}                          % optional, remove / comment the line if not wanted
%\social[twitter]{srvanderplas}               % optional, remove / comment the line if not wanted
\social[github]{srvanderplas}                 % optional, remove / comment the line if not wanted
%\extrainfo{\emailsymbol \emaillink{}}


\begin{document}
\makesimpletitle

\section{Research}

My primary research interests fall into a ``grey area" between statistics, machine learning, and cognitive science. Specifically, I explore ways to increase our ability to understand data, using data visualization and visual statistics; at the same time, I design and use algorithms intended to mimic areas where humans perform better than computers, such as object recognition. This research is inherently interdisciplinary - in order to design algorithms that mimic the human visual system, it is necessary to understand the architecture human visual system in addition to parameter estimation techniques and ways to transform data into relevant feature sets. The bulk of my graduate research focused on the design of graphics optimized for human perception, both in general statistical graphics and in the design of charts for use with large amounts of data in bioinformatics and genetics. My current research in forensics is focused on two areas related to human-statistics interaction, with additional projects in areas related to simulation and software engineering. 

% Human-like algorithms, assessment of man vs. machine performance
The first human-statistics interaction area is in the development and use of human-like machine learning algorithms for forensic pattern evidence. Pattern matching is one area where humans excel and computers are still catching up; however, computer algorithms have advantages in forensics: they are repeatable, auditable, and their failure rates can be determined with relative ease. In addition, algorithms which are perceived to mimic examiners can be explained by analogy to juries and to practitioners; they may allow for an understanding of the process while requiring relatively little statistical theory or notation. I am currently using computer-vision techniques to create feature sets for images of shoes; these feature sets can then be used for modeling, to speed up database searches, or to assess the frequency of certain class characteristics in a local population. In the next 6 months, I plan to use a similar approach to to identify regions of bullets suitable for striae matching; this will increase the efficiency and effectiveness of the bullet matching algorithm by excluding degraded areas from match scores. I also am planning to compare the firearms examiner's process for bullet matching to the automated matching algorithm: if the examiner spends significant time on areas of the bullet that contribute heavily to the match score, the bullet matching algorithm has convergent validity; this may make examiners more comfortable with the automated process and provide additional justification for its use in legal proceedings. 

% Human factors, truthiness, presentation of 
The second human-statistics interaction area I am interested in is what is termed `human factors' in forensics. Specifically, there is concern over the ability of juries and legal professionals to understand and appropriately utilize statistical concepts such as likelihood ratios, Bayes Factors, and error rates to evaluate the weight of forensic evidence. There is also evidence that juries do not necessarily evaluate visual evidence correctly: when presented with brain images from fMRI, they rate the expert presenting those images as more trustworthy and educated, and their claims as more believable. Similar studies have not been done for statistical charts and graphs, but charts and graphs will become more common in legal proceedings as statistical forensics moves from the research lab to the examiner's lab. I am currently laying the groundwork for understanding the effect of charts in legal proceedings with an experiment evaluating how individuals use pictures, charts, and maps to evaluate factual claims. Once we understand how visual aids affect evaluation of generic factual claims, future experiments can add more legal context and examine jurors specifically. I have reached out to individuals doing research into psychology and the law at Iowa State and hope to collaborate with them on future projects. I am also collaborating with Danica Ommen on a grant exploring the relationship between ROC curves, likelihood ratios, and Bayes Factors; I am responsible for developing graphics which visualize these relationships, to be used by statisticians, examiners, and potentially in legal proceedings. Forensic science needs statistics, but we also need to ensure that the statistical information is communicated effectively, so that it can be fairly evaluated and used to improve the justice system instead of introducing a new source of bias. 

% Simulation and data processing
Forensic data, at least as collected at CSAFE, tends to be large, both in size and complexity. Often, this data takes the form of two or three dimensional images, which must then be broken down into meaningful features before statistical analysis can begin. Feature extraction is a process which depends heavily on the domain - for bullets, it is sufficient to identify a single cross-section which best represents the striae; for shoes, there is no simple way to transform 2D data into a single dimension, so feature extraction methods must accommodate additional spatial information. 
% 
% The phrase ``A picture is worth a thousand words" is commonly accepted, in part because the human brain has been optimized over millenia to process visual stimuli quickly and efficiently. We can communicate information much more quickly and effectively using images, which provides a substantial advantage to graphical communication relative to verbal communication methods. This advantage comes with certain drawbacks: statisticians must consider the visual system in order to communicate effectively with charts and graphs. John Tukey and Edward Tufte both described guidelines for data visualization, but there is fairly sparse literature examining the perception of statistical graphics in an experimental setting. My doctoral research considered different situations in which features of a chart or graph might change the conclusion drawn from that chart, even when the underlying data remain the same.
% 
% As statistics becomes more important to the world of forensic science, we have to be able to communicate the results of statistical evaluation of forensic evidence to experts in other fields (lawyers, examiners, judges) as well as to laypeople (juries). If the experts do not understand the statistics, they won't be adopted; if juries cannot understand the statistics, there is little point to using them. Graphics become a valuable tool because they can demonstrate information spatially without requiring the overhead of a complete theoretical understanding of likelihood ratios, hypothesis testing, or evaluation of machine learning algorithms. I am currently working to lay groundwork for studies of how juries evaluate graphs and charts by first understanding how laypeople evaluate factual claims in the presence of pictures, charts, and maps which are either related or unrelated to the factual claim. I am also collaborating on a grant exploring the connection between likelihood ratios, bayes factors, and ROC curves; my responsibility is to create and assess graphics which can be used in legal proceedings to effectively communicate the strength of the evidence to juries. In the future, I hope to collaborate with researchers in the psychology department to assess graphics presented within the context of legal proceedings, with the goal of developing guidelines for both the types of graphics and the languaged used to present them.
% 
% There are certain tasks which humans can still do more effectively than algorithms; for example, object identification, pattern recognition, anomaly detection. In recent years, algorithms have been developed to mimic the architecture of parts of the brain, combining human strengths with computational reproducibility. Convolutional neural networks (CNNs) have become the de facto standard for object recognition in machine learning because they mimic the visual cortex. In forensics, there are certain tasks which are still difficult to automate: identification of randomly acquired marks on shoes, assessment of whether a bullet is suitable for comparison, and labeling of fingerprint minutiae, to name a few. I am working to train a CNN to recognize class characteristics in shoe treads - the resulting identified characteristics can then be used to index shoe models in a database of exemplars; they can also be used to characterize the frequency of certain shoe design elements within a local population. In the coming months, I intend to apply a similar technique to identify areas on bullet scans which are degraded due to pitting, tank rash, or other phenomena. Output from this algorithm can then be used as input to the algorithm used to identify a stable cross section for identification of striae. 
% 
% Even though neural networks are `black box' models, and make statisticians uncomfortable due to their opacity, they can be explained to laypeople by analogy, avoiding the bulk of technical jargon. We need to be able to explain our other algorithms in a similar way, so that practitioners, lawyers, and judges can trust that the statistical algorithms are using data in a way that is similar to the status quo, with additional advantages of being repeatable, auditable, and quantitative. To facilitate this, we need to gather data on what features the examiners use and their importance, and compare those features to the variables used in machine learning models. Predictions from machine learning models can be examined using LIME (local interpretable model-agnostic explanations); on the examiner side, we can use eye-tracking and other tools to gain insight into the features of the evidence used to make a determination. In addition to providing information on the correspondence between the algorithm and the examiner, this investigation may also provide additional features for the next generation of statistical models. Initially, I plan to compare predictions made by the bullet matching algorithm developed at CSAFE with the process used by firearms examiners. I will collect information about the features examiners use to make a determination using a combination of eye tracking, think-aloud protocols, and a virtual comparison microscope; this data will be compared to local features in order to assess the procedural similarity between human and algorithmic approaches. 
% 
% Finally, in any of the pattern evidence areas of forensics, there is a need to obtain sufficient data to characterize the distribution of match scores under known match and known non-match conditions. Typically, this data is difficult, expensive, or time consuming to obtain, so limited sets of data are used to explore the match and non-match distributions. It may not be possible to simulate the input data fully (e.g. generate a 3D simulated bullet scan or shoe print with random characteristics), but for certain stages of any matching algorithm, simulated data may provide additional information about the score distributions. I am currently working on a method to simulate matching and non-matching bullet land signatures using a common examiner test dataset, but I would like to expand that method so that it can be generalized to multiple ammunition and firearm types. To expand the threshold bootstrap method to allow generation of signatures from different ammunition types, it will be necessary to assemble a database of firearm and ammunition types that can be used to characterize the striae features of each combination. From this database, it should then be possible to develop a method by which certain types of striae can be sampled, scaled, and modified, then reassembled into either a set of matching signatures or a set of non-matching signatures. 





\section{Teaching}

\section{Diversity and Inclusion}


\end{document}