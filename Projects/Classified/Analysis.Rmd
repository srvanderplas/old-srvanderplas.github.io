---
title: "Classified Personal Ads"
author: "Susan VanderPlas"
date: "09/08/2014"
output:
  knitrBootstrap::bootstrap_document:
    title: "Cool Packages"
    theme: cerulean
    highlight: sunburst
    theme.chooser: FALSE
    highlight.chooser: FALSE
---

<div class="jumbotron"> 
<h1>Mining Classified Ads </h1>

(a.k.a. there are really creepy people on the internet)</small>
</div>

Disclaimer: This post looks at personal ads on online classifieds. These ads are often sexually explicit and crude, and are likely "Not Safe For Work" in all but the most liberal workplaces. Personal ads may also be known to the state of California to cause cancer. 

# Motivation 
This project was inspired by a [Psychology Today article](http://go.galegroup.com/ps/i.do?id=GALE|A315069654&v=2.1&u=lom_kentdl&it=r&inPS=true&prodId=ITOF&userGroupName=lom_kentdl&p=ITOF) which included the following picture: 

<img src="CraigslistMissedConnectionsByState.jpg" width="100%" height="auto">

I had to wonder... what exactly was going on in Indiana?

# Setup

The data was collected by sampling online personal ads (in English) from random cities in North America at periodic intervals using an R script coupled with a cron job (the frequency of the job changed over time as I tuned the script). Originally, data was written to a MySQL database; the database has been filtered to include only personal ads (of the type one might see [here](http://desmoines.craigslist.org/ppp/)).

```{r setup, message=FALSE, warning=FALSE, echo=-c(1:3), bootstrap.show.code=FALSE}
library(knitr)
opts_chunk$set(fig.align='center', par=TRUE, cache=TRUE, autodep=TRUE, message=FALSE, warning=FALSE, tidy=FALSE, bootstrap.show.code=FALSE)
options(width=150)

library(doMC)
registerDoMC()
library(lubridate)
library(stringr)
library(ggplot2)
```


As this data includes dates and times, we'll use the lubridate package to get these into a more workable format. Similarly, we'll use stringr to manipulate the text of the post more easily. 

# Initial Exploration - Dates, Times, Timezones, and Locations

```{r datesandtimes, message=FALSE, warning=FALSE}
# Read in the data
posts <- read.csv("PersonalAdsPost.csv", row.names=1, stringsAsFactors=FALSE)

# strip timezone information
posts$timezone <- str_sub(posts$post.date, -5, -1)
# posts$post.date2 <- str_sub(posts$post.date, 1, -6)
posts$post.date2 <- posts$post.date
posts$post.date2 <- str_replace(posts$post.date2, "T", " ")
posts$post.date2 <- ymd_hms(posts$post.date2)
posts$post.last.update2 <- str_replace(posts$post.last.update, "T", " ")
posts$post.last.update2 <- ymd_hms(posts$post.last.update)
posts$post.time <- hour(posts$post.date2) + minute(posts$post.date2)/60
posts <- subset(posts, !is.na(post.date2) & !is.na(post.time))
```

Now that we've got workable dates and times, let's plot some basic characteristics of the data. 

```{r clSections, fig.width=10, fig.height=5}
# Relative proportions of posts
posts$post.subcltype2 <- factor(posts$post.subcltype, 
                                levels=c("casual encounters", 
                                         "men seeking men", 
                                         "men seeking women", 
                                         "women seeking men", 
                                         "women seeking women", 
                                         "strictly platonic", 
                                         "miscellaneous romance", 
                                         "missed connections", 
                                         "rants & raves"))
qplot(posts$post.subcltype2) + 
  theme(axis.text.x=element_text(angle=20, hjust=.75)) + 
  xlab("Classified Section") + 
  ylab("Posts") + theme_bw()
```

I was somewhat dissatisfied by the "top 100 posts when the data were collected" explanation in the article (What day was data collected? What time of day? How many days worth of posts were in the top 100, since posts to NYC classifieds are bound to be more numerous than posts to Western Nebraska classifieds?)

In the interests of transparency, here are some basic plots of the "when" of data collection. Times reported are with respect to Central Time, so there are slight discrepancies between Eastern and Pacific time. Lubridate doesn't let you vectorize the time zone argument, though, so this will have to do. 

```{r postsByTimezone, fig.width=10, fig.height=8}
# Posting times, by timezone
posts$timezone2 <- factor(posts$timezone, levels=c("-0500", "-0600", "-0700", "-0800"), labels=c("Eastern", "Central", "Mountain", "Pacific"))
qplot(x=post.time, fill=timezone2, geom="histogram", 
      data=subset(posts, !is.na(timezone2)), binwidth=1) + 
  scale_fill_brewer("Time Zone", palette="Set1") + 
  theme_bw() + 
  facet_grid(timezone2~., scales="free") + 
  scale_x_continuous("Time", limits=c(0, 24), breaks=c(0, 6, 12, 18, 24)) +
  scale_y_continuous("Posts")
```

There are also many fewer posts in the mountain time zone than in other areas. 

```{r postsByDay, fig.width=10, fig.height=5}
# Histogram of post days
qplot(wday(post.date2, label=TRUE, abbr=TRUE), 
      data=subset(posts, !is.na(wday(post.date2)))) + 
  xlab("Day Posted") + ylab("Posts") + theme_bw()
```

Most of the posts were collected on the night of January 14th and 15th (Tuesday and Wednesday, respectively), when I was testing to see how far I could get my script to scale (pretty far, as it turns out; at the end I was scraping 20-30 randomly selected cities every 10 minutes. Duplicate posts were removed, but the bias is still very obvious.)

```{r postsByDate, fig.width=10, fig.height=4}
# Density of posts
qplot(post.date2, geom="density", data=posts) + 
  xlab("Post Date") + 
  ylab("Density") + 
  theme_bw()
```

Data collection was conducted in January, and shortly thereafter I had to do a distribution upgrade on the server and the script was broken afterwards (likely due to formatting changes on the site in question). Once the spring semester started, I didn't have time to fix the script, so we have about 6 weeks worth of data to explore, collected between January 14-15. 

The skewed distribution here is a result of the way the online classified site works: I scraped the top 300 posts from each randomly selected location. In more populated areas, 300 posts might represent activity from a couple of days; in less populated areas, 300 posts might represent an entire month of activity. 

```{r postSpan, fig.width=6, fig.height=10}
library(dplyr)

post.location <- group_by(posts, cityurl) %>% 
  summarize(min_date=min(post.date2), 
            max_date=max(post.date2))

# remove web address
post.location$city <- str_replace(post.location$cityurl, "http://", "")
post.location$city <- str_replace(post.location$city, "\\.(en\\.)?(\\w*)\\.(org)?(ca)?", "")

posts$city <- str_replace(posts$cityurl, "http://", "")
posts$city <- str_replace(posts$city, "\\.(en\\.)?(\\w*)\\.(org)?(ca)?", "")

qplot(x=min_date, xend=max_date, y=city, yend=city, 
      geom="segment", data=post.location) + 
  xlab("Range of Collected Posts") + ylab("City/Region") + 
  ggtitle("Time Coverage of Classified Ads") + 
  geom_point(data=posts, aes(x=post.date2, y=city), 
             alpha=.2, inherit.aes=FALSE) + theme_bw()
```

When we plot the total number of posts in our database by classified location, we can see that there are between 300 and 600 posts per City/Region. A large city which has many posts that happened to be sampled twice during the process (as I sampled with replacement for this project) might have more than 300 unique posts, where a more rural area which had fewer posts might have only 300 unique posts (fewer if links were broken). 

```{r clPostDistribution, fig.width=6, fig.height=10}
qplot(x=city, geom="histogram", data=posts) + coord_flip() + theme_bw() + xlab("City/Region") + ylab("Posts Collected")
```

I'm currently re-running the sampling script in the hopes that I can get both a longer time-period (on the order of a month or so of data) and more locales. Hopefully, there will be more data (with a more even distribution) in the next few weeks... then we can finally settle this!

# Words, words, words - Text analysis of posts and titles
I'll use the tm package to get a sense of the text in the posts. We'll start by looking at the most common words in the corpus (the collection of all posts). 
```{r textminingsetup}
library(tm)

# need to clean up text before putting it into tm_map...
posts$post.text2 <- str_replace_all(posts$post.text, "[\'\"]", "")
posts$post.text2 <- str_replace_all(posts$post.text2, "[[:punct:]]", " ")
posts$post.text2 <- str_replace_all(posts$post.text2, "\\n", " ")
posts$post.text2 <- str_replace_all(posts$post.text2, "[[:digit:]]", " ")

# create the corpus
myCorpus <- Corpus(VectorSource(posts$post.text2))

dict <- unlist(lapply(tolower(posts$post.text2), strsplit, " "))
dict.table <- table(dict)
dict.table <- dict.table[order(dict.table, decreasing=TRUE)]

# Most common words: 
names(dict.table[1:20])
# conclusion: we need to remove stopwords. 
```

## Frequent Terms

We'll use functions in the tm package to remove stopwords, punctuation, and numbers from the corpus. We will then take the corrected text and make a matrix associating terms with the texts they appear in. 

```{r textminingcorpus}
# transform the content, removing stopwords, numbers, and punctuation
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, content_transformer(removePunctuation))
myCorpus <- tm_map(myCorpus, content_transformer(removeNumbers))
myStopwords <- stopwords('english')
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

# Create a DocumentTermMatrix 
myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf)))
findFreqTerms(myDtm, lowfreq=800) # terms in at least 800 posts (total)
```

## Word Associations

Now that the document term matrix has been created, we can look at word associations. As these are personal ads, let's see what words are associated with the word "attractive": 
```{r wordassociations-attractive}
findAssocs(myDtm, "attractive", .125)
```
The results are thoroughly confusing. Moving on... what about "single"?
```{r wordassociations-single}
findAssocs(myDtm, "single", .10)
```
"Single" is associated with "married", "orgy", "appriciated" (I presume they meant appreciated), "family", etc. Clearly, association is not the same as synonym. 

Let's try something that should produce a more crude response: words associated with "long". 
```{r wordassociations-long}
findAssocs(myDtm, "long", .15)
```
No surprises there. Or were you thinking dirty already? 

I also wonder if "married" is going to be as confusing as "single"? 
```{r wordassociations-married}
findAssocs(myDtm, "married", .1)
```
Ok, so "married" isn't quite as confusing, even if people can't spell "discrete". 

Personal Ads are known for using abbreviations (even when space isn't restricted, as is the case online). "nsa" stands for "No Strings Attached" - let's see what it's associated with (we won't have the resolution necessary to determine whether it's associated with the National Security Administration). 
```{r wordassociations-nsa}
findAssocs(myDtm, "nsa", .075)
```
Odd; seems that it's associated with "fwb" (friends with benefits) and many words that appear to be concatenated - "earlythree", "andnine", etc.

A few terms chosen to be a bit superficial (weight, appearance, etc.)
```{r wordassociations-lbs}
findAssocs(myDtm, "lbs", .1)
```
Ok, posts with "lbs" also specify appearance. That makes sense. 

Another appearance-related term; its associations will probably not surprise you. 
```{r wordassociations-thick}
findAssocs(myDtm, "thick", .1)
```


## Measuring Dirty Words

### Is there a dictionary for this kind of thing?

Yes, yes, there is. In fact, I think there are many. In an effort to avoid scraping UrbanDictionary, we'll go with the (current) [first search hit](http://cltampa.com/dailyloaf/archives/2012/09/16/dirty-sex-dictionary?showFullText=true) on google for "dirty words dictionary". 

```{r dirtywords}
# Scrape Dirty Words list

url <- "http://cltampa.com/dailyloaf/archives/2012/09/16/dirty-sex-dictionary?showFullText=true"

library(scrapeR)
library(stringr)

site <- scrape(url)[[1]]

defs <- tolower(sapply(getNodeSet(site, "//*[@id='BlogsBlogPost']/div[1]/div[2]/p"), xmlValue))
defs <- subset(defs, nchar(defs)>3)
defs <- defs[!grepl("jump to:", defs)]

sex.synonyms <- defs[grepl("sex: ask your mother.", defs)]
defs[grepl("sex: ask your mother.", defs)] <- "sex: ask your mother. apparently she is an expert on the subject."
sex.synonyms <- gsub("sex: ask your mother. apparently she is an expert on the subject. sex.synonyms: ", "", sex.synonyms)
sex.synonyms <- str_split(sex.synonyms, pattern="(([[:alpha:]]|#): )|(, )")[[1]]
sex.synonyms <- sex.synonyms[!grepl(":", sex.synonyms) & nchar(sex.synonyms)>1]
sex.synonyms <- paste0(sex.synonyms, ": sex")

guy.stuff <- gsub("^\\s|(\\.\\.\\.)", "", paste(unlist(str_split(gsub("^[[:alpha:]]:", "", defs[grepl("^[[:alpha:]]:", defs)]), pattern = ", ")), ": male masturbation", sep=""))
defs <- defs[!grepl("^[[:alpha:]]:", defs)]

girl.stuff <- paste0(str_trim(str_split(word(defs[grepl("—female ", defs)], sep=":", 2), ",")[[1]]), ": ", "female masturbation")
defs <- defs[!grepl("—female ", defs)]


problem.entries <- subset(defs, str_count(defs, ":")>1)

# Fix entries with synonyms at the end by repeating the definition and adding a new line for each synonym. (Not perfect, but it'll do)
synonyms <- problem.entries[grepl("(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", problem.entries)]
problem.entries <- problem.entries[!grepl("(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", problem.entries)]

term <- str_split(word(synonyms, sep="(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", start=-1), ",")
term.def <- str_replace(word(synonyms, sep=":", 2), "(synonym(s)?(y)?)|(variation(s)?)|(related to)|(similar( to)?)|(also)", "")
synonyms.fixed <- gsub("\\.:", ":", unlist(lapply(1:length(term), function(i) paste0(str_replace(unlist(term[i]), "^(\\s)+|\\.$", ""), ":", term.def[i]))))
synonyms.fixed <- synonyms.fixed[!grepl("^:", synonyms.fixed)]
synonyms.fixed <- gsub(":$", "", synonyms.fixed)
rm(term, term.def)

# Remove "origins" at the end of definition
origins <- problem.entries[grepl("origins:", problem.entries)]
origins <- word(origins, sep="origin")
problem.entries <- problem.entries[!grepl("origins:", problem.entries)]

# Remove other addenda
problem.entries <- word(problem.entries, sep="usage:")
problem.entries <- word(problem.entries, sep="antonym(s)?:")
problem.entries <- word(problem.entries, sep="pop-culture reference(s)?:")
problem.entries <- word(problem.entries, sep="quote:")
problem.entries <- word(problem.entries, sep="see:")
problem.entries <- word(problem.entries, sep=": see")
problem.entries <- word(problem.entries, sep="history: ")

# Split definitions that aren't separated
notseparated <- problem.entries[grepl("\\.\\w*( \\w*)?:", problem.entries)]
problem.entries <- problem.entries[!grepl("\\.\\w*( \\w*)?:", problem.entries)]

notseparated <- c(word(notseparated, sep="\\.\\w*( \\w*)?:"), gsub("^\\.", "", str_extract(notseparated, "\\.\\w*( \\w*)?:.*$")))

defs2 <- c(defs, synonyms, synonyms.fixed, sex.synonyms, girl.stuff, guy.stuff, origins, problem.entries)
rm(list=ls()[ls()!="defs2"])

df <- data.frame(word = word(defs2, sep=":", 1))
df$definition <- str_replace(defs2, paste0(df$word, ":"), "")

```

We can then use this dictionary (or at least, the list of words and phrases) and look for occurrences in our document matrix. 

```{r dictionarySearch}
library(slam)
myTerms <- str_trim(gsub("[[:punct:]]", " ", gsub(",", " ", gsub("//", " ", tolower(df$word)))))
dirtyDTM <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf), dictionary=myTerms))

# keep only words that occurred at least 15 times in the corpus
dirtyDTM <- dirtyDTM[,which(col_sums(dirtyDTM)>=15)]
```


## Topic Modeling
We'll use the mallet package and LDAvis to try to model the various topics in Craigslist personal ads. I'm following the [LDAvis documentation](https://cpsievert.github.io/LDAvis/reviews/reviews.html) and examples (written by the most awesome Carson Sievert) to complete this step. 

```{r malletLDAvis}
# Download stopwords
download.file("http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop", "stopwords.txt")
library(mallet)
instance <- mallet.import(as.character(posts$post.title), as.character(posts$post.text), "stopwords.txt")

# Start with 9 topics, since there are 9 categories to post in
model <- MalletLDA(num.topics=9)
model$loadDocuments(instance)
freqs <- mallet.word.freqs(model)

# Fitting/training the model
model$train(2000)
# Compute the topic-term distribution (smoothed=TRUE includes prior's effect)
phi <- t(mallet.topic.words(model, smoothed = TRUE, normalized = TRUE))
phi.count <- t(mallet.topic.words(model, smoothed = TRUE, normalized = FALSE))
topic.words <- mallet.topic.words(model, smoothed = TRUE, normalized = FALSE)
topic.counts <- rowSums(topic.words)
topic.proportions <- topic.counts/sum(topic.counts)
vocab <- model$getVocabulary()

library(LDAvis)
out <- check.inputs(K = 25, W = length(vocab), phi = phi, 
                    term.frequency = apply(phi.count, 1, sum), 
                    vocab = vocab, topic.proportion = topic.proportions)
# Relabel topics so that topics are numbered in decreasing order of frequency.
colnames(out$phi) <- seq_len(out$K)

# Create json file
json <- with(out, createJSON(K = 25, phi, term.frequency, 
                   vocab, topic.proportion))

# Create visualization

serVis(json, out.dir = 'vis', open.browser = FALSE)
```

<iframe src = "vis/index.html" width=900 height=700></iframe>
