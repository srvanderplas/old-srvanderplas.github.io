<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

  <head>

    <meta charset="utf-8" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="generator" content="pandoc" />

    
    
    <title></title>

        <script src="Analysis_files/jquery-1.11.0/jquery.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link href="Analysis_files/bootstrap3-3.2.0/css/bootstrap.min.css" rel="stylesheet" />
    <link href="Analysis_files/bootstrap3-3.2.0/css/themes/cerulean/bootstrap.min.css" rel="stylesheet" />
    <script src="Analysis_files/bootstrap3-3.2.0/js/bootstrap.min.js"></script>
    <link href="Analysis_files/highlightjs-8.2/highlight/sunburst.css" rel="stylesheet" />
    <script src="Analysis_files/highlightjs-8.2/highlight.pack.js"></script>
    <link href="Analysis_files/MagnificPopup-0.9.9/magnific-popup.css" rel="stylesheet" />
    <script src="Analysis_files/MagnificPopup-0.9.9/magnific-popup.js"></script>
    <link href="Analysis_files/knitrBootstrap-0.0.1/css/knitrBootstrap.css" rel="stylesheet" />
    <script src="Analysis_files/knitrBootstrap-0.0.1/js/knitrBootstrap.js"></script>
    
    
  </head>

  <body data-spy="scroll" data-target="#toc">
    <div id="wrapper" class="container">
      <div class="row">
        <div class="contents col-xs-12 col-md-10">

          
                    
<!----
html_document
---><div class="jumbotron"><h1>Mining Classified Ads</h1>
<small>(a.k.a. there are really creepy people on the internet)</small></div><p>Disclaimer: This post looks at personal ads on online classifieds. These ads are often sexually explicit and crude, and are likely “Not Safe For Work” in all but the most liberal workplaces.</p><div id="motivation" class="section level1"><h1>Motivation</h1><p>This project was inspired by a <a href="http://go.galegroup.com/ps/i.do?id=GALE|A315069654&amp;v=2.1&amp;u=lom_kentdl&amp;it=r&amp;inPS=true&amp;prodId=ITOF&amp;userGroupName=lom_kentdl&amp;p=ITOF">Psychology Today article</a> which included the following picture:</p><p><img src="CraigslistMissedConnectionsByState.jpg" width="100%" height="auto"></p><p>I had to wonder… what exactly was going on in Indiana?</p></div><div id="setup" class="section level1"><h1>Setup</h1><p>The data was collected by sampling online personal ads (in English) from random cities in North America at periodic intervals using an R script coupled with a cron job (the frequency of the job changed over time as I tuned the script). Originally, data was written to a MySQL database; the database has been filtered to include only personal ads (of the type one might see <a href="http://desmoines.craigslist.org/ppp/">here</a>).</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">library(doMC)
registerDoMC()
library(lubridate)
library(stringr)
library(ggplot2)
library(plyr)
library(dplyr)</code></pre></div><p>As this data includes dates and times, we’ll use the lubridate package to get these into a more workable format. Similarly, we’ll use stringr to manipulate the text of the post more easily.</p></div><div id="initial-exploration---dates-times-timezones-and-locations" class="section level1"><h1>Initial Exploration - Dates, Times, Timezones, and Locations</h1><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Read in the data
posts <- read.csv("PersonalAdsPost.csv", row.names=1, stringsAsFactors=FALSE)

# strip timezone information
posts$timezone <- str_sub(posts$post.date, -5, -1)
# posts$post.date2 <- str_sub(posts$post.date, 1, -6)
posts$post.date2 <- posts$post.date
posts$post.date2 <- str_replace(posts$post.date2, "T", " ")
posts$post.date2 <- ymd_hms(posts$post.date2)
posts$post.last.update2 <- str_replace(posts$post.last.update, "T", " ")
posts$post.last.update2 <- ymd_hms(posts$post.last.update)
posts$post.time <- hour(posts$post.date2) + minute(posts$post.date2)/60
posts <- subset(posts, !is.na(post.date2) & !is.na(post.time))</code></pre></div><p>Now that we’ve got workable dates and times, let’s plot some basic characteristics of the data.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Relative proportions of posts
posts$post.subcltype2 <- factor(posts$post.subcltype, 
                                levels=c("casual encounters", 
                                         "men seeking men", 
                                         "men seeking women", 
                                         "women seeking men", 
                                         "women seeking women", 
                                         "strictly platonic", 
                                         "miscellaneous romance", 
                                         "missed connections", 
                                         "rants & raves"))
qplot(posts$post.subcltype2) + 
  theme(axis.text.x=element_text(angle=20, hjust=.75)) + 
  xlab("Classified Section") + 
  ylab("Posts") + theme_bw()</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/clSections-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>I was somewhat dissatisfied by the “top 100 posts when the data were collected” explanation in the article (What day was data collected? What time of day? How many days worth of posts were in the top 100, since posts to NYC classifieds are bound to be more numerous than posts to Western Nebraska classifieds?)</p><p>In the interests of transparency, here are some basic plots of the “when” of data collection. Times reported are with respect to Central Time, so there are slight discrepancies between Eastern and Pacific time. Lubridate doesn’t let you vectorize the time zone argument, though, so this will have to do.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Posting times, by timezone
posts$timezone2 <- factor(posts$timezone, levels=c("-0500", "-0600", "-0700", "-0800"), labels=c("Eastern", "Central", "Mountain", "Pacific"))
qplot(x=post.time, fill=timezone2, geom="histogram", 
      data=subset(posts, !is.na(timezone2)), binwidth=1) + 
  scale_fill_brewer("Time Zone", palette="Set1") + 
  theme_bw() + 
  facet_grid(timezone2~., scales="free") + 
  scale_x_continuous("Time", limits=c(0, 24), breaks=c(0, 6, 12, 18, 24)) +
  scale_y_continuous("Posts")</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/postsByTimezone-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>There are also many fewer posts in the mountain time zone than in other areas.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Histogram of post days
qplot(wday(post.date2, label=TRUE, abbr=TRUE), 
      data=subset(posts, !is.na(wday(post.date2)))) + 
  xlab("Day Posted") + ylab("Posts") + theme_bw()</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/postsByDay-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>Most of the posts were collected on the night of January 14th and 15th (Tuesday and Wednesday, respectively), when I was testing to see how far I could get my script to scale (pretty far, as it turns out; at the end I was scraping 20-30 randomly selected cities every 10 minutes. Duplicate posts were removed, but the bias is still very obvious.)</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Density of posts
qplot(post.date2, geom="density", data=posts) + 
  xlab("Post Date") + 
  ylab("Density") + 
  theme_bw()</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/postsByDate-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>Data collection was conducted in January, and shortly thereafter I had to do a distribution upgrade on the server and the script was broken afterwards (likely due to formatting changes on the site in question). Once the spring semester started, I didn’t have time to fix the script, so we have about 6 weeks worth of data to explore, collected between January 14-15.</p><p>The skewed distribution here is a result of the way the online classified site works: I scraped the top 300 posts from each randomly selected location. In more populated areas, 300 posts might represent activity from a couple of days; in less populated areas, 300 posts might represent an entire month of activity.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">post.location <- group_by(posts, cityurl) %>% 
  summarize(min_date=min(post.date2), 
            max_date=max(post.date2))

# remove web address
post.location$city <- str_replace(post.location$cityurl, "http://", "")
post.location$city <- str_replace(post.location$city, "\\.(en\\.)?(\\w*)\\.(org)?(ca)?", "")

posts$city <- str_replace(posts$cityurl, "http://", "")
posts$city <- str_replace(posts$city, "\\.(en\\.)?(\\w*)\\.(org)?(ca)?", "")

qplot(x=min_date, xend=max_date, y=city, yend=city, 
      geom="segment", data=post.location) + 
  xlab("Range of Collected Posts") + ylab("City/Region") + 
  ggtitle("Time Coverage of Classified Ads") + 
  geom_point(data=posts, aes(x=post.date2, y=city), 
             alpha=.2, inherit.aes=FALSE) + theme_bw()</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/postSpan-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>When we plot the total number of posts in our database by classified location, we can see that there are between 300 and 600 posts per City/Region. A large city which has many posts that happened to be sampled twice during the process (as I sampled with replacement for this project) might have more than 300 unique posts, where a more rural area which had fewer posts might have only 300 unique posts (fewer if links were broken).</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">qplot(x=city, geom="histogram", data=posts) + coord_flip() + theme_bw() + xlab("City/Region") + ylab("Posts Collected")</code></pre><div class="row"><div class="col-md-offset-3 col-md-6"><a href="#" class="thumbnail"><img src="Analysis_files/figure-html/clPostDistribution-1.png" title="" alt="" style="display: block; margin: auto;" /></a></div></div></div><p>I’m currently re-running the sampling script in the hopes that I can get both a longer time-period (on the order of a month or so of data) and more locales. Hopefully, there will be more data (with a more even distribution) in the next few weeks… then we can finally settle this!</p></div><div id="words-words-words" class="section level1"><h1>Words, words, words</h1>I’ll use the tm package to get a sense of the text in the posts. We’ll start by looking at the most common words in the corpus (the collection of all posts).<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">library(tm)

# need to clean up text before putting it into tm_map...
posts$post.text2 <- str_replace_all(posts$post.text, "[\'\"]", "")
posts$post.text2 <- str_replace_all(posts$post.text2, "[[:punct:]]", " ")
posts$post.text2 <- str_replace_all(posts$post.text2, "\\n", " ")
posts$post.text2 <- str_replace_all(posts$post.text2, "[[:digit:]]", " ")

# create the corpus
myCorpus <- Corpus(VectorSource(posts$post.text2))

dict <- unlist(lapply(tolower(posts$post.text2), strsplit, " "))
dict.table <- table(dict)
dict.table <- dict.table[order(dict.table, decreasing=TRUE)]

# Most common words: 
names(dict.table[1:20])</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##  [1] ""        "and"     "to"      "a"       "i"       "you"     "for"     "the"     "in"      "looking" "me"     
## [12] "im"      "be"      "with"    "or"      "if"      "have"    "your"    "of"      "am"
</code></pre>
<button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># conclusion: we need to remove stopwords. </code></pre></div><div id="frequent-terms" class="section level2"><h2>Frequent Terms</h2><p>We’ll use functions in the tm package to remove stopwords, punctuation, and numbers from the corpus. We will then take the corrected text and make a matrix associating terms with the texts they appear in.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># transform the content, removing stopwords, numbers, and punctuation
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, content_transformer(removePunctuation))
myCorpus <- tm_map(myCorpus, content_transformer(removeNumbers))
myStopwords <- stopwords('english')
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

# Create a DocumentTermMatrix 
myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf)))
findFreqTerms(myDtm, lowfreq=1500) # terms in at least 1500 posts (total)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##  [1] "age"        "also"       "around"     "ass"        "back"       "bi"         "big"        "body"      
##  [9] "can"        "clean"      "cock"       "come"       "couple"     "cum"        "cut"        "ddf"       
## [17] "dick"       "discreet"   "dont"       "email"      "face"       "find"       "first"      "free"      
## [25] "fuck"       "fun"        "get"        "girl"       "give"       "go"         "good"       "guy"       
## [33] "guys"       "hit"        "host"       "hot"        "ill"        "im"         "interested" "just"      
## [41] "know"       "lbs"        "let"        "lets"       "like"       "line"       "little"     "long"      
## [49] "looking"    "love"       "make"       "male"       "man"        "married"    "maybe"      "meet"      
## [57] "must"       "need"       "new"        "nice"       "now"        "old"        "older"      "one"       
## [65] "open"       "oral"       "pic"        "pics"       "play"       "please"     "put"        "real"      
## [73] "really"     "reply"      "right"      "see"        "send"       "sex"        "someone"    "something" 
## [81] "stats"      "subject"    "suck"       "take"       "time"       "want"       "well"       "white"     
## [89] "will"       "woman"      "women"      "work"       "youre"
</code></pre></div></div><div id="word-associations" class="section level2"><h2>Word Associations</h2>Now that the document term matrix has been created, we can look at word associations. As these are personal ads, let’s see what words are associated with the word “attractive”:<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "attractive", .125)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##                attractive
## addressed            0.13
## authenticated        0.13
## communicates         0.13
## cradar               0.13
## craigs               0.13
## deniability          0.13
## dillsburg            0.13
## elects               0.13
## enable               0.13
## identification       0.13
## identifying          0.13
## identities           0.13
## impulsively          0.13
## impulsiveness        0.13
## naming               0.13
## plausible            0.13
## recourses            0.13
## safeguard            0.13
## sitters              0.13
## thoughtfulness       0.13
## thusday              0.13
## unproductive         0.13
</code></pre></div>The results are thoroughly confusing. Moving on… what about “single”?<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "single", .10)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##              single
## appriciated    0.15
## homophobes     0.15
## children       0.13
## married        0.13
## orgy           0.13
## annual         0.12
## family         0.12
## male           0.12
## relationship   0.12
## specify        0.12
## dont           0.10
## flaggers       0.10
## honest         0.10
</code></pre></div><p>“Single” is associated with “married”, “orgy”, “appriciated” (I presume they meant appreciated), “family”, etc. Clearly, association is not the same as synonym.</p>Let’s try something that should produce a more crude response: words associated with “long”.<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "long", .15)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##              long
## term         0.43
## relationship 0.19
## lasting      0.16
## dont         0.15
## know         0.15
## time         0.15
</code></pre></div><p>No surprises there. Or were you thinking dirty already?</p>I also wonder if “married” is going to be as confusing as “single”?<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "married", .1)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##              married
## single          0.13
## wife            0.13
## discreet        0.12
## looking         0.12
## attached        0.11
## bi              0.11
## relationship    0.11
## situation       0.11
## woman           0.11
## dont            0.10
## just            0.10
## kids            0.10
</code></pre></div><p>Ok, so “married” isn’t quite as confusing.</p>Personal Ads are known for using abbreviations (even when space isn’t restricted, as is the case online). “nsa” stands for “No Strings Attached” - let’s see what it’s associated with (we won’t have the resolution necessary to determine whether it’s associated with the National Security Administration).<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "nsa", .075)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##               nsa
## fun          0.13
## fwb          0.11
## afun         0.08
## andnine      0.08
## bathe        0.08
## birdtwo      0.08
## carter       0.08
## earlythree   0.08
## friendlies   0.08
## geteight     0.08
## hotone       0.08
## investments  0.08
## looking      0.08
## prematurely  0.08
## theseven     0.08
## transmission 0.08
## withand      0.08
## yro          0.08
</code></pre></div><p>Odd; seems that it’s associated with “fwb” (friends with benefits) and many words that appear to be concatenated - “earlythree”, “andnine”, etc.</p>A few terms chosen to be a bit superficial (weight, appearance, etc.)<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "lbs", .1)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##        lbs
## ft    0.20
## cut   0.16
## hair  0.15
## brown 0.12
## blue  0.11
## eyes  0.11
</code></pre></div><p>Ok, posts with “lbs” also specify appearance. That makes sense.</p>Another appearance-related term; its associations will probably not surprise you.<div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">findAssocs(myDtm, "thick", .1)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">##        thick
## cock    0.16
## cut     0.12
## inches  0.12
</code></pre></div></div><div id="generating-classified-posts-pictures-not-included" class="section level2"><h2>Generating Classified Posts (pictures not included!)</h2><p>We can use our posts to generate new posts as well. Using n-grams (strings of <span class="math">\(n\)</span> words), we can generate new posts that are based on the conditional probability that “this” follows “that”. Specifically, if <span class="math">\(x\)</span> is a string of <span class="math">\(n\)</span> words and <span class="math">\(y\)</span> is the <span class="math">\((n+1)\)</span>th word, we are modeling <span class="math">\(P(y | x)\)</span>, that is, the probability that <span class="math">\(y\)</span> is next after <span class="math">\(x\)</span>. So if we’re modeling 2-grams, and the last 2 words we read were “Just in”, we’d likely consider “case” and “time” to have high probabilities of being the next word.</p><p>The <code>ngram</code> package splits our text up into ngrams and then recombines the ngrams to form an entirely new text using the <code>babble()</code> function. We’ll use the functions in the ngram package to create new craigslist ads. I’ve added tokens to the beginning and end of each ad, to help differentiate how ads start from where they end. The tokens are structured in such a way that there are several “words” that mark the beginning and end of a post; that way, strings that are not related should be definitively separate even using 6-grams (I think up to using 10-grams, actually). I have not split punctuation from words, so only words which ended sentances in the database of posts will be available to end sentances in the generated text.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">library(ngram)

post.len <- as.numeric(sapply(as.character(posts$post.text), wordcount))


clean.posts <- function(text, space.words=FALSE){
  # Remove web addresses, since the punctuation will get all messed up anyways.
  text <- str_replace_all(text, "http:\\\\[a-zA-Z1-9]{1,}\\.[a-zA-Z]{1,4}", " ")
  # Make parentheses spacing uniform _(_ ..... _)_
  text <- str_replace_all(text, "([\\()])", " \\1 ")
  # Enforce spaces between punctuation but allow for multiple punctuation marks in a row
  text <- str_replace_all(text, "([\\.,\\?!]{1,})", " \\1 ")
  if(space.words){
    text <- str_replace_all(text, " ([\\.,\\?!]{1})", "\\1")
  }
  # Remove all endline characters
  text <- str_replace_all(text, "\\n", " ")
  # replace all incidences of multiple spaces with a single space
  text <- str_replace_all(text, "( ){2,}", " ")
  # Remove all non-ascii characters
  text <- iconv(text, "", "ASCII//TRANSLIT")
  # make all text lower case
  text <- tolower(text)
  
  text
}

# Clean posts and then collapse all posts into a single string where individual posts are marked with special tags
babble.text <- clean.posts(posts$post.text)
babble.text <- paste(" - - - - BEGIN ", babble.text)
babble.text <- paste(" END - - - - ", babble.text)
babble.text <- concat(babble.text, collapse = " ")

# preprocess text
babble.text <- preprocess(babble.text, case=NULL)

# Create 2-grams
ngram.res <- ngram(babble.text, n=2)

# Looking at the object
print(ngram.res)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">## [1] "An ngram object with 309138 2-grams"
</code></pre>
<button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Want to ensure complete sentances (not like the originals) and also break ngrams apart by BEGIN and END
fix.babble <- function(text){
  if(length(text)==1){
    if(grepl("(END|BEGIN)", text)){
      gen.posts <- str_trim(str_split(text, "END (- ){1,}BEGIN")[[1]])
      gen.posts <- gen.posts[-c(1,length(gen.posts))]
    } else {
      gen.posts <- str_trim(text)
    }
  } else {
    gen.posts <- str_trim(text)
  }
  fixed <- str_replace_all(gen.posts, " ([\\.,\\?!\\()]{1,}) ", "\\1 ")
  return(fixed)
}

# Function to compare generated posts to original posts
library(plyr)
compare.posts <- function(post){
    ldply(post, function(i) {
    first.words <- str_extract(i, "([[:alnum:]]{1,}[ (\\. )]){6,8}")
    if(is.na(first.words) | length(first.words)==0){
      return(data.frame(generated.post=i, original.match=NA, stringsAsFactors=FALSE))
    }
    match <- which(grepl(first.words, tolower(posts$post.text)))
    if(length(match)>0){
      return(data.frame(generated.post=i, original.match=unique(fix.babble(clean.posts(posts$post.text[match]))), stringsAsFactors=FALSE))
    } else {
      return(data.frame(generated.post=i, original.match=NA, stringsAsFactors=FALSE))
    } 
  })
}

out.posts <- babble(ngram.res, 1000, seed=241205828)
out.posts <- fix.babble(out.posts)
compare <- compare.posts(out.posts)</code></pre></div><blockquote><p>i am of thursday in the afternoon in v. just looking for now. safe only. send a face pic because im an average gal, big cocks open to all types of music……. but attraction does apply !</p></blockquote><blockquote><p>i am willing to fool around with occasionally. of course - not in the subject line .</p></blockquote><p>This sounds only slightly more schizophrenic than the typical Craigslist ad. Just for fun, let’s ramp up our game and use 3-grams in our model. Conditioning on an extra word should give us more sentance continuity, so that the topic doesn’t change back and forth quite so much.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Create 3-grams
ngram.res <- ngram(babble.text, n=3)

# Generate a new sequence of 1000 words
out.posts <- fix.babble(babble(ngram.res, 1000, seed=109897276))
compare <- compare.posts(out.posts)</code></pre></div><blockquote><p>i’m new around lubbock and need new friends. enclose a picture with your reply and i’ll send it. i’m looking for something hot in the face ?</p></blockquote><blockquote><p>i am a virgin so you can go live your wonderful new life now. i am very secure in the relationship i have with my master. i will meet up with a new person, i don’t do the bar thing i dont do or like lol race age and weight dont matter lol so dont be afraid to reply.. just try it one more time, i’m free till 4 so get at me if you are an employee of a home body but would like to relieve some stress. send a pic www. youtube. com/watch? v=ufchjjfg1rm</p></blockquote><p>These are still quite prone to rambling - the originals are a bit better, at least. We still haven’t increased complexity to the point where we’re running the risk of duplicating ads from the database - this would occur if we used a larger value for <span class="math">\(n\)</span> in our <span class="math">\(n\)</span>-grams. We aren’t even matching the first 6-8 words in any post: of the 10 posts we generated, we got 3 matches back.</p><p>Just for fun, we’ll try something crazy and use 6-grams. This is likely to get us much closer to things that are actually in the classified ads, but 6-grams <a href="http://wuglife.tumblr.com/post/74088955204/lovebible-pl-a-markov-generator-trained-on-h-p">have been previously used for this sort of task with some success</a> (that link is pretty sweet - King James Bible + HP Lovecraft generated text).</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Create 6-grams
ngram.res <- ngram(babble.text, n=6)

# Looking at the object
print(ngram.res)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">## [1] "An ngram object with 1521246 6-grams"
</code></pre>
<button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Generate a new sequence of 500 words
out.posts <- fix.babble(babble(ngram.res, 500, seed=609807796))

compare <- compare.posts(out.posts)</code></pre></div><p>Giving 6-grams a try, we get some pretty decent generated classified ads. They’re at least as coherent as the source text…</p><blockquote><p>looking for something new. married man looking for a new taste of a woman. 35yo 180lb athletic build. looking for a cute sweet woman that is not looking for any strings. im clean and i have the medical records to prove it. please no weirdos or drama queens. if we hit it off, we can do it more often. age is not a problem and the older the better but not over 70 but need to be attractive. over 19 only as well. i can not host, do not reply if you can’t send a face pic in place of a dick pic because i wont be looking at a dick when i first meet you lol. 18-30 with a boyish look is definitely a plus. hope to hear from you. thanks .</p></blockquote><p>Comparing the first few words of this to real ads gets us</p><blockquote><p>looking for something new. married man looking for a new taste of a woman. 35yo 180lb athletic build. looking for a cute sweet woman that is not looking for any strings. im clean and i have the medical records to prove it. please no weirdos or drama queens. if we hit it off and all is good, maybe we could make the taste continue. respond with no ropes in subject and include a clean pic .</p></blockquote><p>So there is a little mixing of ads, but not too much.</p><blockquote><p>have pics upon request, never been with a guy yet looking to experiment with an older top man. looking for wednesday, must send pics</p></blockquote><p>Searching the database for the first few words provides the same ad in the original database:</p><blockquote><p>have pics upon request, never been with a guy yet looking to experiment with an older top man. looking for wednesday, must send pics</p></blockquote><p>So our generator is a bit too specific here (or we need a larger database).</p><blockquote><p>older guy 6ft good build 6“cut looking for a young small smooth bottom ver’s to give me a bj and bottom for me and maybe top if it’s goes that far. must be discreet and ddf and reply with pic. this is for today not late tonight .</p></blockquote><p>Again, this post is pretty coherent (or at least, par for the course in this dataset). Checking our database, we find the same ad:</p><blockquote><p>older guy 6ft good build 6“cut looking for a young small smooth bottom ver’s to give me a bj and bottom for me and maybe top if it’s goes that far. must be discreet and ddf and reply with pic. this is for today not late tonight .</p></blockquote><p>So 6-grams are probably a bit much. Of the 7 posts that resulted from our 500 word sequence, 4 of them are nearly verbatim copies of posts in our database. Of course, part of this depends on the length of the post - a short post is going to be much more likely to be verbatim from the database, because there are so few transitions between n-grams.</p><p>Moving backwards a bit, we can examine 4-grams in the same way to see whether they are better.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Create 4-grams
ngram.res <- ngram(babble.text, n=4)

# Looking at the object
print(ngram.res)</code></pre>
<button class="output R toggle btn btn-xs btn-success"><span class="glyphicon glyphicon-chevron-down"></span> R output</button>
<pre style=""><code class="output r">## [1] "An ngram object with 1204717 4-grams"
</code></pre>
<button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Generate a new sequence of 500 words
out.posts <- fix.babble(babble(ngram.res, 1000, seed=90345987))

compare <- compare.posts(out.posts)</code></pre></div><p>We’re searching our database for the first 6-8 words in the generated post; any matches should contain those words. Looking at our results, the first sequence is not too bad - starts off the same, but transitions nicely.</p><blockquote><p>i’m a hot &amp; sexy girl. are there any ladies out there like giving head? i almost never get it at home, and i am an extremely open minded, fetish friendly, out calls only some of my specialties include, but are not limited to: foot/shoe fetish showers sadism bdsm feminization toy play sensory deprivation &amp; more!! couples welcome, NA</p></blockquote><p>The rest of the 12 posts that comprise the 1000 word sequence we generated aren’t so bad either.</p><blockquote><p>all horned up tonight with only your hand for relief? don’t waste that cum, marine! nine1zero. male cocksucker supplying nsa hummer with no charge and no drama. looking for a gentleman that would be 1. attracted to them 2. treat them nicely 3. love them for who they are 4. promise something long term in return 5. is actually fun to be with. if you have any hobbies i would love to talk and meet whoever. in paradise, i dnt have money to spend for gas to chico and back. agian im looking for a woman….!!!!!!!! so please put in headliner so i know your real, attach a pic and i’ll send one back.. lets do this asap guys !!, all horned up tonight with only your hand for relief? don’t waste that cum, marine! nine1zero. male cocksucker supplying nsa hummer with no charge and no drama. 5four6. hosting off western so just get here and get head, or i can travel to you. two8two3. discretion and privacy guaranteed, so hmu asap while the ad’s still up .</p></blockquote><p>Of the 12, 0 are perfect matches to the database, and 4 don’t even match the first 6-8 words!</p></div><div id="measuring-dirty-words" class="section level2"><h2>Measuring Dirty Words</h2><div id="is-there-a-dictionary-for-this-kind-of-thing" class="section level3"><h3>Is there a dictionary for this kind of thing?</h3><p>Yes, yes, there is. In fact, I think there are many. In an effort to avoid scraping UrbanDictionary, we’ll go with the (current) <a href="http://cltampa.com/dailyloaf/archives/2012/09/16/dirty-sex-dictionary?showFullText=true">first search hit</a> on google for “dirty words dictionary”.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r"># Scrape Dirty Words list

url <- "http://cltampa.com/dailyloaf/archives/2012/09/16/dirty-sex-dictionary?showFullText=true"

library(scrapeR)
library(stringr)

site <- scrape(url)[[1]]

defs <- tolower(sapply(getNodeSet(site, "//*[@id='BlogsBlogPost']/div[1]/div[2]/p"), xmlValue))
defs <- subset(defs, nchar(defs)>3)
defs <- defs[!grepl("jump to:", defs)]

sex.synonyms <- defs[grepl("sex: ask your mother.", defs)]
defs[grepl("sex: ask your mother.", defs)] <- "sex: ask your mother. apparently she is an expert on the subject."
sex.synonyms <- gsub("sex: ask your mother. apparently she is an expert on the subject. sex.synonyms: ", "", sex.synonyms)
sex.synonyms <- str_split(sex.synonyms, pattern="(([[:alpha:]]|#): )|(, )")[[1]]
sex.synonyms <- sex.synonyms[!grepl(":", sex.synonyms) & nchar(sex.synonyms)>1]
sex.synonyms <- paste0(sex.synonyms, ": sex")

guy.stuff <- gsub("^\\s|(\\.\\.\\.)", "", paste(unlist(str_split(gsub("^[[:alpha:]]:", "", defs[grepl("^[[:alpha:]]:", defs)]), pattern = ", ")), ": male masturbation", sep=""))
defs <- defs[!grepl("^[[:alpha:]]:", defs)]

girl.stuff <- paste0(str_trim(str_split(word(defs[grepl("—female ", defs)], sep=":", 2), ",")[[1]]), ": ", "female masturbation")
defs <- defs[!grepl("—female ", defs)]


problem.entries <- subset(defs, str_count(defs, ":")>1)

# Fix entries with synonyms at the end by repeating the definition and adding a new line for each synonym. (Not perfect, but it'll do)
synonyms <- problem.entries[grepl("(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", problem.entries)]
problem.entries <- problem.entries[!grepl("(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", problem.entries)]

term <- str_split(word(synonyms, sep="(synonym(s)?(y)?:)|(variation(s)?:)|(related to:)|(similar( to)?:)|(also:)", start=-1), ",")
term.def <- str_replace(word(synonyms, sep=":", 2), "(synonym(s)?(y)?)|(variation(s)?)|(related to)|(similar( to)?)|(also)", "")
synonyms.fixed <- gsub("\\.:", ":", unlist(lapply(1:length(term), function(i) paste0(str_replace(unlist(term[i]), "^(\\s)+|\\.$", ""), ":", term.def[i]))))
synonyms.fixed <- synonyms.fixed[!grepl("^:", synonyms.fixed)]
synonyms.fixed <- gsub(":$", "", synonyms.fixed)
rm(term, term.def)

# Remove "origins" at the end of definition
origins <- problem.entries[grepl("origins:", problem.entries)]
origins <- word(origins, sep="origin")
problem.entries <- problem.entries[!grepl("origins:", problem.entries)]

# Remove other addenda
problem.entries <- word(problem.entries, sep="usage:")
problem.entries <- word(problem.entries, sep="antonym(s)?:")
problem.entries <- word(problem.entries, sep="pop-culture reference(s)?:")
problem.entries <- word(problem.entries, sep="quote:")
problem.entries <- word(problem.entries, sep="see:")
problem.entries <- word(problem.entries, sep=": see")
problem.entries <- word(problem.entries, sep="history: ")

# Split definitions that aren't separated
notseparated <- problem.entries[grepl("\\.\\w*( \\w*)?:", problem.entries)]
problem.entries <- problem.entries[!grepl("\\.\\w*( \\w*)?:", problem.entries)]

notseparated <- c(word(notseparated, sep="\\.\\w*( \\w*)?:"), gsub("^\\.", "", str_extract(notseparated, "\\.\\w*( \\w*)?:.*$")))

defs2 <- c(defs, synonyms, synonyms.fixed, sex.synonyms, girl.stuff, guy.stuff, origins, problem.entries)
rm(notseparated, problem.entries, origins, synonyms.fixed, synonyms, defs, sex.synonyms, girl.stuff, guy.stuff)

df <- data.frame(word = word(defs2, sep=":", 1))
df$definition <- str_replace(defs2, paste0(df$word, ":"), "")</code></pre></div><p>We can then use this dictionary (or at least, the list of words and phrases) and look for occurrences in our document matrix.</p><div class="row"><button class="source R toggle btn btn-xs btn-primary"><span class="glyphicon glyphicon-chevron-up"></span> R source</button>
<pre style="display:none"><code class="source r">library(slam)
myTerms <- str_trim(gsub("[[:punct:]]", " ", gsub(",", " ", gsub("//", " ", tolower(df$word)))))
dirtyDTM <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf), dictionary=myTerms))

# keep only words that occurred at least 15 times in the corpus
dirtyDTM <- dirtyDTM[,which(col_sums(dirtyDTM)>=15)]</code></pre></div><!----
## Topic Modeling
We'll use the mallet package and LDAvis to try to model the various topics in Craigslist personal ads. I'm following the [LDAvis documentation](https://cpsievert.github.io/LDAvis/reviews/reviews.html) and examples (written by the most awesome Carson Sievert) to complete this step. 



 <iframe src = "vis/index.html" width=900 height=700></iframe>
--->

</div></div></div>

          <p class="text-muted" id="credit">
            Styled with <a href="https://github.com/jimhester/knitrBootstrap">knitrBootstrap</a>
          </p>

        </div>

                <div  class="col-md-2">
          <div class="hidden-print hidden-xs hidden-sm sidebar affix">
            <div id="toc" class="sidenav well">
              <ul><li><a href="#motivation">Motivation</a></li><li><a href="#setup">Setup</a></li><li><a href="#initial-exploration---dates-times-timezones-and-locations">Initial Exploration - Dates, Times, Timezones, and Locations</a></li><li><a href="#words-words-words">Words, words, words</a><ul><li><a href="#frequent-terms">Frequent Terms</a></li><li><a href="#word-associations">Word Associations</a></li><li><a href="#generating-classified-posts-pictures-not-included">Generating Classified Posts (pictures not included!)</a></li><li><a href="#measuring-dirty-words">Measuring Dirty Words</a></li></ul></li></ul>
            </div>
          </div>
        </div>
        
      </div>
    </div>

        <!-- dynamically load mathjax for compatibility with self-contained -->
    <script>
      (function () {
          var script = document.createElement("script");
          script.type = "text/javascript";
          script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
          document.getElementsByTagName("head")[0].appendChild(script);
      })();
    </script>
      </body>
</html>
